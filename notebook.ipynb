{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import os   \n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from cap_dataset import CascadeRegression\n",
    "from cap_model import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/dev/Repos/CAPP\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_regression_ds = CascadeRegression(root=\"data\", name=\"facebook\", edge_index_path=\"data/raw/facebook/adj.txt\", task=\"regression\", observation=3)\n",
    "fb_classification_ds = CascadeRegression(root=\"data\", name=\"facebook\", edge_index_path=\"data/raw/facebook/adj.txt\", task=\"classification\", observation=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4039, 4], edge_index=[2, 176468], y=[1], cascade_name='1')\n",
      "tensor([1.3868e-02, 3.9042e-06, 1.5299e-05, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "curr_ds = fb_regression_ds\n",
    "print(curr_ds[1])\n",
    "print(curr_ds[1].x[1645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/miniconda3/envs/ml/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/Users/rezatabrizi/miniconda3/envs/ml/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GAT(fb_regression_ds.num_features, 64, 1, 8)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/miniconda3/envs/ml/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "total_size = len(fb_regression_ds)\n",
    "train_size = int(0.7 * total_size)\n",
    "valid_size = int(0.15 * total_size) \n",
    "test_size = total_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split (fb_regression_ds, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrse_loss(output, target):\n",
    "    if target != 0:\n",
    "        loss = ((output - target) / target) ** 2\n",
    "    else:\n",
    "        loss = (output - target) ** 2\n",
    "    return loss\n",
    "\n",
    "def msre_loss_batch(out, target):\n",
    "    nonzero_mask = (target != 0)\n",
    "\n",
    "    loss = torch.zeros_like(out)\n",
    "    loss[nonzero_mask] = ((out[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask]) ** 2\n",
    "    loss[~nonzero_mask] = (out[~nonzero_mask] - target[~nonzero_mask]) ** 2\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def mape_loss_batch(out, target):\n",
    "    nonzero_mask = (target != 0)\n",
    "\n",
    "    loss = torch.zeros_like(out)\n",
    "    loss[nonzero_mask] = (torch.abs(out[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask])\n",
    "    loss[~nonzero_mask] = (torch.abs(out[~nonzero_mask] - target[~nonzero_mask]))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def wroperc_error(out, target, epsilon):\n",
    "    nonzero_mask = (target != 0)\n",
    "\n",
    "    loss = torch.zeros_like(out, dtype=torch.float)\n",
    "    loss[nonzero_mask] = ((torch.abs(out[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask]) >= epsilon).float()\n",
    "    loss[~nonzero_mask] = (torch.abs(out[~nonzero_mask] - target[~nonzero_mask]) >= epsilon).float()\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainLoader):\n",
    "    mrse_total_loss = 0.0\n",
    "    mrse_running_loss = 0.0\n",
    "    mape_total_loss = 0.0\n",
    "    mape_running_loss = 0.0\n",
    "    wroperc_total_loss = 0.0\n",
    "    wroperc_running_loss = 0.0\n",
    "    model.train()\n",
    "    n = len(trainLoader)\n",
    "    for idx, data in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        current_MRSE = msre_loss_batch(out, data.y)\n",
    "        current_MAPE = mape_loss_batch(out, data.y)\n",
    "        current_wroperc = wroperc_error(out, data.y, 0.5)\n",
    "\n",
    "        mrse_total_loss += current_MRSE.item()\n",
    "        mrse_running_loss += current_MRSE.item()\n",
    "        mape_total_loss += current_MAPE.item()\n",
    "        mape_running_loss += current_MAPE.item()\n",
    "        wroperc_total_loss += current_wroperc.item()\n",
    "        wroperc_running_loss += current_wroperc.item()\n",
    "\n",
    "        current_MRSE.backward()\n",
    "        optimizer.step()\n",
    "        if (idx+1) % 40 == 0:\n",
    "            print(f\"Batch {idx+1}, MRSE Loss: {mrse_running_loss/40:.2f}\")\n",
    "            mrse_running_loss = 0.0\n",
    "            mape_running_loss = 0.0\n",
    "            wroperc_running_loss = 0.0\n",
    "\n",
    "    return mrse_total_loss / n, mape_total_loss / n, wroperc_total_loss / n\n",
    "\n",
    "\n",
    "def test(testLoader):\n",
    "    model.eval()\n",
    "    mrse_total_loss = 0\n",
    "    mape_total_loss = 0.0\n",
    "    wroperc_total_loss = 0.0\n",
    "    n = len(testLoader)\n",
    "    with torch.no_grad():\n",
    "        for data in testLoader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "            current_MRSE = msre_loss_batch(out, data.y)\n",
    "            current_MAPE = mape_loss_batch(out, data.y)\n",
    "            current_wroperc = wroperc_error(out, data.y, 0.5)\n",
    "\n",
    "            mrse_total_loss += current_MRSE.item()\n",
    "            mape_total_loss += current_MAPE.item()\n",
    "            wroperc_total_loss += current_wroperc.item()\n",
    "            \n",
    "    return mrse_total_loss / n, mape_total_loss / n, wroperc_total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40, MRSE Loss: 13400.46\n",
      "Batch 80, MRSE Loss: 0.76\n",
      "Batch 120, MRSE Loss: 0.72\n",
      "Batch 160, MRSE Loss: 0.74\n",
      "Batch 200, MRSE Loss: 0.75\n",
      "Batch 240, MRSE Loss: 0.73\n",
      "Batch 280, MRSE Loss: 0.72\n",
      "Batch 320, MRSE Loss: 0.72\n",
      "Epoch 1: Train Loss: 1629.8815, Val MRSE Loss: 0.7848, Val MAPE Loss: 0.8804, Val WroPerc: 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    mrse_loss, mape_loss, wroperc_loss = train(train_loader)\n",
    "    v_mrse_loss, v_mape_loss, v_wroperc_loss = test(valid_loader)\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {mrse_loss:.4f}, Val MRSE Loss: {v_mrse_loss:.4f}, Val MAPE Loss: {v_mape_loss:.4}, Val WroPerc: {v_wroperc_loss:.4}')\n",
    "\n",
    "t_mrse_loss, t_mape_loss, t_wroperc_loss = test(test_dataset)\n",
    "print(f'Test MRSE Loss: {t_mrse_loss:.4f}, Test MAPE Loss: {t_mape_loss:.4}, Test WroPerc: {t_wroperc_loss:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reduce head to have better performance\n",
    "2. Run 100 epochs \n",
    "3. Use graph norm \n",
    "4. See how coupledgnn data is \n",
    "5. see if there is an issue with the y and the output\n",
    "\n",
    "Lab meetin 5/04 \n",
    "1. Remove graph pooling and just sum the nx1 matrix after decoding. After decoding apply sigmoid. \n",
    "2. Download weibo dataset and preprocess\n",
    "3. Remove activation between layers. Only needed at the end. \n",
    "4. make sure code is right because it is only possible to get fluctutation if your code is not right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
