{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch_geometric) (1.25.2)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from torch_geometric) (1.8.0)\n",
      "Collecting fsspec (from torch_geometric)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch_geometric) (3.0.3)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /usr/lib/python3/dist-packages (from torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from torch_geometric) (0.23.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/lib/python3/dist-packages (from torch_geometric) (5.9.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch_geometric)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m155.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: multidict, fsspec, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 torch_geometric-2.5.3 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import os   \n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from cap_dataset import CascadeRegression\n",
    "from cap_model import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Repos/CAPP\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "fb_regression_ds = CascadeRegression(root=\"data\", name=\"facebook\", edge_index_path=\"data/raw/facebook/adj.txt\", task=\"regression\", observation=3)\n",
    "fb_classification_ds = CascadeRegression(root=\"data\", name=\"facebook\", edge_index_path=\"data/raw/facebook/adj.txt\", task=\"classification\", observation=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4039, 4], edge_index=[2, 176468], y=[1], cascade_name='1')\n",
      "tensor([1.3868e-02, 3.9042e-06, 1.5299e-05, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "curr_ds = fb_regression_ds\n",
    "print(curr_ds[1])\n",
    "print(curr_ds[1].x[1645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GAT(fb_regression_ds.num_features, 64, 1, 8)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "total_size = len(fb_regression_ds)\n",
    "train_size = int(0.7 * total_size)\n",
    "valid_size = int(0.15 * total_size) \n",
    "test_size = total_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split (fb_regression_ds, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrse_loss(output, target):\n",
    "    if target != 0:\n",
    "        loss = ((output - target) / target) ** 2\n",
    "    else:\n",
    "        loss = (output - target) ** 2\n",
    "    return loss\n",
    "\n",
    "def msre_loss_batch(out, target):\n",
    "    nonzero_mask = (target != 0)\n",
    "\n",
    "    loss = torch.zeros_like(out)\n",
    "    loss[nonzero_mask] = ((out[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask]) ** 2\n",
    "    loss[~nonzero_mask] = (out[~nonzero_mask] - target[~nonzero_mask]) ** 2\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def mape_loss_batch(out, target):\n",
    "    nonzero_mask = (target != 0)\n",
    "\n",
    "    loss = torch.zeros_like(out)\n",
    "    loss[nonzero_mask] = (torch.abs(out[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask])\n",
    "    loss[~nonzero_mask] = (torch.abs(out[~nonzero_mask] - target[~nonzero_mask]))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def wroperc_error(out, target, epsilon):\n",
    "    nonzero_mask = (target != 0)\n",
    "\n",
    "    loss = torch.zeros_like(out, dtype=torch.float)\n",
    "    loss[nonzero_mask] = ((torch.abs(out[nonzero_mask] - target[nonzero_mask]) / target[nonzero_mask]) >= epsilon).float()\n",
    "    loss[~nonzero_mask] = (torch.abs(out[~nonzero_mask] - target[~nonzero_mask]) >= epsilon).float()\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainLoader):\n",
    "    mrse_total_loss = 0.0\n",
    "    mrse_running_loss = 0.0\n",
    "    mape_total_loss = 0.0\n",
    "    mape_running_loss = 0.0\n",
    "    wroperc_total_loss = 0.0\n",
    "    wroperc_running_loss = 0.0\n",
    "    model.train()\n",
    "    n = len(trainLoader)\n",
    "    for idx, data in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        current_MRSE = msre_loss_batch(out, data.y)\n",
    "        current_MAPE = mape_loss_batch(out, data.y)\n",
    "        current_wroperc = wroperc_error(out, data.y, 0.5)\n",
    "\n",
    "        mrse_total_loss += current_MRSE.item()\n",
    "        mrse_running_loss += current_MRSE.item()\n",
    "        mape_total_loss += current_MAPE.item()\n",
    "        mape_running_loss += current_MAPE.item()\n",
    "        wroperc_total_loss += current_wroperc.item()\n",
    "        wroperc_running_loss += current_wroperc.item()\n",
    "\n",
    "        current_MRSE.backward()\n",
    "        optimizer.step()\n",
    "        if (idx+1) % 40 == 0:\n",
    "            print(f\"Batch {idx+1}, MRSE Loss: {mrse_running_loss/40:.2f}\")\n",
    "            mrse_running_loss = 0.0\n",
    "            mape_running_loss = 0.0\n",
    "            wroperc_running_loss = 0.0\n",
    "\n",
    "    return mrse_total_loss / n, mape_total_loss / n, wroperc_total_loss / n\n",
    "\n",
    "\n",
    "def test(testLoader):\n",
    "    model.eval()\n",
    "    mrse_total_loss = 0\n",
    "    mape_total_loss = 0.0\n",
    "    wroperc_total_loss = 0.0\n",
    "    n = len(testLoader)\n",
    "    with torch.no_grad():\n",
    "        for data in testLoader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "\n",
    "            current_MRSE = msre_loss_batch(out, data.y)\n",
    "            current_MAPE = mape_loss_batch(out, data.y)\n",
    "            current_wroperc = wroperc_error(out, data.y, 0.5)\n",
    "\n",
    "            mrse_total_loss += current_MRSE.item()\n",
    "            mape_total_loss += current_MAPE.item()\n",
    "            wroperc_total_loss += current_wroperc.item()\n",
    "            \n",
    "    return mrse_total_loss / n, mape_total_loss / n, wroperc_total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40, MRSE Loss: 6888.18\n",
      "Batch 80, MRSE Loss: 0.50\n",
      "Batch 120, MRSE Loss: 0.51\n",
      "Batch 160, MRSE Loss: 0.49\n",
      "Batch 200, MRSE Loss: 0.49\n",
      "Batch 240, MRSE Loss: 0.45\n",
      "Batch 280, MRSE Loss: 0.44\n",
      "Batch 320, MRSE Loss: 0.47\n",
      "Epoch 1: Train Loss: 837.8888, Val MRSE Loss: 0.5211, Val MAPE Loss: 0.673, Val WroPerc: 0.7218\n",
      "Batch 40, MRSE Loss: 0.44\n",
      "Batch 80, MRSE Loss: 0.42\n",
      "Batch 120, MRSE Loss: 0.41\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.39\n",
      "Batch 280, MRSE Loss: 0.37\n",
      "Batch 320, MRSE Loss: 0.41\n",
      "Epoch 2: Train Loss: 0.4049, Val MRSE Loss: 0.4160, Val MAPE Loss: 0.5587, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.37\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.37\n",
      "Batch 160, MRSE Loss: 0.37\n",
      "Batch 200, MRSE Loss: 0.38\n",
      "Batch 240, MRSE Loss: 0.38\n",
      "Batch 280, MRSE Loss: 0.43\n",
      "Batch 320, MRSE Loss: 0.40\n",
      "Epoch 3: Train Loss: 0.3871, Val MRSE Loss: 0.4034, Val MAPE Loss: 0.5513, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.39\n",
      "Batch 80, MRSE Loss: 0.33\n",
      "Batch 120, MRSE Loss: 0.41\n",
      "Batch 160, MRSE Loss: 0.41\n",
      "Batch 200, MRSE Loss: 0.43\n",
      "Batch 240, MRSE Loss: 0.37\n",
      "Batch 280, MRSE Loss: 0.38\n",
      "Batch 320, MRSE Loss: 0.39\n",
      "Epoch 4: Train Loss: 0.3861, Val MRSE Loss: 0.4078, Val MAPE Loss: 0.5542, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.37\n",
      "Batch 80, MRSE Loss: 0.42\n",
      "Batch 120, MRSE Loss: 0.42\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.36\n",
      "Batch 240, MRSE Loss: 0.38\n",
      "Batch 280, MRSE Loss: 0.36\n",
      "Batch 320, MRSE Loss: 0.40\n",
      "Epoch 5: Train Loss: 0.3873, Val MRSE Loss: 0.4038, Val MAPE Loss: 0.552, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.42\n",
      "Batch 80, MRSE Loss: 0.38\n",
      "Batch 120, MRSE Loss: 0.41\n",
      "Batch 160, MRSE Loss: 0.41\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.35\n",
      "Batch 280, MRSE Loss: 0.38\n",
      "Batch 320, MRSE Loss: 0.35\n",
      "Epoch 6: Train Loss: 0.3883, Val MRSE Loss: 0.4115, Val MAPE Loss: 0.5568, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.43\n",
      "Batch 80, MRSE Loss: 0.38\n",
      "Batch 120, MRSE Loss: 0.35\n",
      "Batch 160, MRSE Loss: 0.41\n",
      "Batch 200, MRSE Loss: 0.37\n",
      "Batch 240, MRSE Loss: 0.38\n",
      "Batch 280, MRSE Loss: 0.38\n",
      "Batch 320, MRSE Loss: 0.41\n",
      "Epoch 7: Train Loss: 0.3876, Val MRSE Loss: 0.4029, Val MAPE Loss: 0.5519, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.39\n",
      "Batch 80, MRSE Loss: 0.36\n",
      "Batch 120, MRSE Loss: 0.41\n",
      "Batch 160, MRSE Loss: 0.41\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.37\n",
      "Batch 280, MRSE Loss: 0.36\n",
      "Batch 320, MRSE Loss: 0.39\n",
      "Epoch 8: Train Loss: 0.3869, Val MRSE Loss: 0.4042, Val MAPE Loss: 0.5495, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.41\n",
      "Batch 80, MRSE Loss: 0.42\n",
      "Batch 120, MRSE Loss: 0.35\n",
      "Batch 160, MRSE Loss: 0.43\n",
      "Batch 200, MRSE Loss: 0.37\n",
      "Batch 240, MRSE Loss: 0.35\n",
      "Batch 280, MRSE Loss: 0.40\n",
      "Batch 320, MRSE Loss: 0.38\n",
      "Epoch 9: Train Loss: 0.3872, Val MRSE Loss: 0.3996, Val MAPE Loss: 0.5476, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.37\n",
      "Batch 80, MRSE Loss: 0.37\n",
      "Batch 120, MRSE Loss: 0.38\n",
      "Batch 160, MRSE Loss: 0.43\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.37\n",
      "Batch 280, MRSE Loss: 0.43\n",
      "Batch 320, MRSE Loss: 0.35\n",
      "Epoch 10: Train Loss: 0.3883, Val MRSE Loss: 0.4195, Val MAPE Loss: 0.5601, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.38\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.38\n",
      "Batch 160, MRSE Loss: 0.37\n",
      "Batch 200, MRSE Loss: 0.38\n",
      "Batch 240, MRSE Loss: 0.42\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.38\n",
      "Epoch 11: Train Loss: 0.3865, Val MRSE Loss: 0.4028, Val MAPE Loss: 0.5487, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.40\n",
      "Batch 80, MRSE Loss: 0.38\n",
      "Batch 120, MRSE Loss: 0.36\n",
      "Batch 160, MRSE Loss: 0.38\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.39\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.41\n",
      "Epoch 12: Train Loss: 0.3896, Val MRSE Loss: 0.4120, Val MAPE Loss: 0.5574, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.39\n",
      "Batch 80, MRSE Loss: 0.39\n",
      "Batch 120, MRSE Loss: 0.43\n",
      "Batch 160, MRSE Loss: 0.37\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.37\n",
      "Batch 280, MRSE Loss: 0.37\n",
      "Batch 320, MRSE Loss: 0.38\n",
      "Epoch 13: Train Loss: 0.3861, Val MRSE Loss: 0.4170, Val MAPE Loss: 0.559, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.41\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.41\n",
      "Batch 160, MRSE Loss: 0.37\n",
      "Batch 200, MRSE Loss: 0.35\n",
      "Batch 240, MRSE Loss: 0.40\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.37\n",
      "Epoch 14: Train Loss: 0.3880, Val MRSE Loss: 0.4179, Val MAPE Loss: 0.559, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.37\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.37\n",
      "Batch 160, MRSE Loss: 0.39\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.34\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.42\n",
      "Epoch 15: Train Loss: 0.3873, Val MRSE Loss: 0.4124, Val MAPE Loss: 0.5566, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.40\n",
      "Batch 80, MRSE Loss: 0.34\n",
      "Batch 120, MRSE Loss: 0.38\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.33\n",
      "Batch 240, MRSE Loss: 0.41\n",
      "Batch 280, MRSE Loss: 0.41\n",
      "Batch 320, MRSE Loss: 0.43\n",
      "Epoch 16: Train Loss: 0.3888, Val MRSE Loss: 0.4107, Val MAPE Loss: 0.5561, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.41\n",
      "Batch 80, MRSE Loss: 0.42\n",
      "Batch 120, MRSE Loss: 0.44\n",
      "Batch 160, MRSE Loss: 0.39\n",
      "Batch 200, MRSE Loss: 0.35\n",
      "Batch 240, MRSE Loss: 0.36\n",
      "Batch 280, MRSE Loss: 0.36\n",
      "Batch 320, MRSE Loss: 0.37\n",
      "Epoch 17: Train Loss: 0.3893, Val MRSE Loss: 0.4019, Val MAPE Loss: 0.5506, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.39\n",
      "Batch 80, MRSE Loss: 0.38\n",
      "Batch 120, MRSE Loss: 0.37\n",
      "Batch 160, MRSE Loss: 0.36\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.38\n",
      "Batch 280, MRSE Loss: 0.38\n",
      "Batch 320, MRSE Loss: 0.43\n",
      "Epoch 18: Train Loss: 0.3868, Val MRSE Loss: 0.4008, Val MAPE Loss: 0.5481, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.41\n",
      "Batch 80, MRSE Loss: 0.36\n",
      "Batch 120, MRSE Loss: 0.40\n",
      "Batch 160, MRSE Loss: 0.39\n",
      "Batch 200, MRSE Loss: 0.36\n",
      "Batch 240, MRSE Loss: 0.37\n",
      "Batch 280, MRSE Loss: 0.40\n",
      "Batch 320, MRSE Loss: 0.43\n",
      "Epoch 19: Train Loss: 0.3878, Val MRSE Loss: 0.4072, Val MAPE Loss: 0.5549, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.44\n",
      "Batch 80, MRSE Loss: 0.36\n",
      "Batch 120, MRSE Loss: 0.40\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.38\n",
      "Batch 240, MRSE Loss: 0.40\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.34\n",
      "Epoch 20: Train Loss: 0.3894, Val MRSE Loss: 0.4074, Val MAPE Loss: 0.5501, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.37\n",
      "Batch 80, MRSE Loss: 0.42\n",
      "Batch 120, MRSE Loss: 0.39\n",
      "Batch 160, MRSE Loss: 0.39\n",
      "Batch 200, MRSE Loss: 0.35\n",
      "Batch 240, MRSE Loss: 0.40\n",
      "Batch 280, MRSE Loss: 0.40\n",
      "Batch 320, MRSE Loss: 0.39\n",
      "Epoch 21: Train Loss: 0.3873, Val MRSE Loss: 0.4073, Val MAPE Loss: 0.5541, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.41\n",
      "Batch 80, MRSE Loss: 0.37\n",
      "Batch 120, MRSE Loss: 0.38\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.34\n",
      "Batch 280, MRSE Loss: 0.38\n",
      "Batch 320, MRSE Loss: 0.39\n",
      "Epoch 22: Train Loss: 0.3879, Val MRSE Loss: 0.4107, Val MAPE Loss: 0.5567, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.38\n",
      "Batch 80, MRSE Loss: 0.38\n",
      "Batch 120, MRSE Loss: 0.35\n",
      "Batch 160, MRSE Loss: 0.43\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.35\n",
      "Batch 280, MRSE Loss: 0.40\n",
      "Batch 320, MRSE Loss: 0.42\n",
      "Epoch 23: Train Loss: 0.3868, Val MRSE Loss: 0.4021, Val MAPE Loss: 0.5523, Val WroPerc: 0.5282\n",
      "Batch 40, MRSE Loss: 0.34\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.41\n",
      "Batch 160, MRSE Loss: 0.39\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.37\n",
      "Batch 280, MRSE Loss: 0.38\n",
      "Batch 320, MRSE Loss: 0.41\n",
      "Epoch 24: Train Loss: 0.3870, Val MRSE Loss: 0.3929, Val MAPE Loss: 0.5463, Val WroPerc: 0.5282\n",
      "Batch 40, MRSE Loss: 0.41\n",
      "Batch 80, MRSE Loss: 0.41\n",
      "Batch 120, MRSE Loss: 0.37\n",
      "Batch 160, MRSE Loss: 0.42\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.38\n",
      "Batch 280, MRSE Loss: 0.36\n",
      "Batch 320, MRSE Loss: 0.39\n",
      "Epoch 25: Train Loss: 0.3886, Val MRSE Loss: 0.4078, Val MAPE Loss: 0.5523, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.34\n",
      "Batch 80, MRSE Loss: 0.44\n",
      "Batch 120, MRSE Loss: 0.40\n",
      "Batch 160, MRSE Loss: 0.38\n",
      "Batch 200, MRSE Loss: 0.35\n",
      "Batch 240, MRSE Loss: 0.41\n",
      "Batch 280, MRSE Loss: 0.43\n",
      "Batch 320, MRSE Loss: 0.37\n",
      "Epoch 26: Train Loss: 0.3883, Val MRSE Loss: 0.4100, Val MAPE Loss: 0.5557, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.38\n",
      "Batch 80, MRSE Loss: 0.37\n",
      "Batch 120, MRSE Loss: 0.39\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.40\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.37\n",
      "Epoch 27: Train Loss: 0.3889, Val MRSE Loss: 0.4175, Val MAPE Loss: 0.5557, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.39\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.37\n",
      "Batch 160, MRSE Loss: 0.37\n",
      "Batch 200, MRSE Loss: 0.37\n",
      "Batch 240, MRSE Loss: 0.42\n",
      "Batch 320, MRSE Loss: 0.38\n",
      "Epoch 28: Train Loss: 0.3885, Val MRSE Loss: 0.4076, Val MAPE Loss: 0.5529, Val WroPerc: 0.5669\n",
      "Batch 40, MRSE Loss: 0.36\n",
      "Batch 80, MRSE Loss: 0.37\n",
      "Batch 120, MRSE Loss: 0.42\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.42\n",
      "Batch 280, MRSE Loss: 0.37\n",
      "Batch 320, MRSE Loss: 0.38\n",
      "Epoch 29: Train Loss: 0.3878, Val MRSE Loss: 0.4044, Val MAPE Loss: 0.5536, Val WroPerc: 0.5282\n",
      "Batch 40, MRSE Loss: 0.35\n",
      "Batch 80, MRSE Loss: 0.39\n",
      "Batch 120, MRSE Loss: 0.39\n",
      "Batch 160, MRSE Loss: 0.40\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.38\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.43\n",
      "Epoch 30: Train Loss: 0.3906, Val MRSE Loss: 0.4350, Val MAPE Loss: 0.5634, Val WroPerc: 0.5915\n",
      "Batch 40, MRSE Loss: 0.43\n",
      "Batch 80, MRSE Loss: 0.39\n",
      "Batch 120, MRSE Loss: 0.39\n",
      "Batch 160, MRSE Loss: 0.33\n",
      "Batch 200, MRSE Loss: 0.40\n",
      "Batch 240, MRSE Loss: 0.40\n",
      "Batch 280, MRSE Loss: 0.39\n",
      "Batch 320, MRSE Loss: 0.38\n",
      "Epoch 31: Train Loss: 0.3889, Val MRSE Loss: 0.3978, Val MAPE Loss: 0.5491, Val WroPerc: 0.5282\n",
      "Batch 40, MRSE Loss: 0.37\n",
      "Batch 80, MRSE Loss: 0.40\n",
      "Batch 120, MRSE Loss: 0.43\n",
      "Batch 160, MRSE Loss: 0.37\n",
      "Batch 200, MRSE Loss: 0.39\n",
      "Batch 240, MRSE Loss: 0.36\n",
      "Batch 280, MRSE Loss: 0.40\n",
      "Batch 320, MRSE Loss: 0.37\n",
      "Epoch 32: Train Loss: 0.3863, Val MRSE Loss: 0.4161, Val MAPE Loss: 0.5556, Val WroPerc: 0.5775\n",
      "Batch 40, MRSE Loss: 0.34\n",
      "Batch 80, MRSE Loss: 0.34\n",
      "Batch 120, MRSE Loss: 0.21\n",
      "Batch 160, MRSE Loss: 0.27\n",
      "Batch 200, MRSE Loss: 0.28\n",
      "Batch 240, MRSE Loss: 0.26\n",
      "Batch 280, MRSE Loss: 0.25\n",
      "Batch 320, MRSE Loss: 0.22\n",
      "Epoch 33: Train Loss: 0.2706, Val MRSE Loss: 0.3142, Val MAPE Loss: 0.5086, Val WroPerc: 0.5106\n",
      "Batch 40, MRSE Loss: 0.21\n",
      "Batch 80, MRSE Loss: 0.20\n",
      "Batch 120, MRSE Loss: 0.23\n",
      "Batch 160, MRSE Loss: 0.20\n",
      "Batch 200, MRSE Loss: 0.18\n",
      "Batch 240, MRSE Loss: 0.23\n",
      "Batch 280, MRSE Loss: 0.20\n",
      "Batch 320, MRSE Loss: 0.20\n",
      "Epoch 34: Train Loss: 0.2068, Val MRSE Loss: 0.1791, Val MAPE Loss: 0.3417, Val WroPerc: 0.2394\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.20\n",
      "Batch 120, MRSE Loss: 0.21\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.22\n",
      "Batch 240, MRSE Loss: 0.20\n",
      "Batch 280, MRSE Loss: 0.21\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 35: Train Loss: 0.1889, Val MRSE Loss: 0.1635, Val MAPE Loss: 0.3281, Val WroPerc: 0.2289\n",
      "Batch 40, MRSE Loss: 0.18\n",
      "Batch 80, MRSE Loss: 0.20\n",
      "Batch 120, MRSE Loss: 0.20\n",
      "Batch 160, MRSE Loss: 0.20\n",
      "Batch 200, MRSE Loss: 0.18\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.19\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 36: Train Loss: 0.1830, Val MRSE Loss: 0.1865, Val MAPE Loss: 0.3598, Val WroPerc: 0.25\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.19\n",
      "Batch 120, MRSE Loss: 0.20\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.20\n",
      "Batch 280, MRSE Loss: 0.25\n",
      "Batch 320, MRSE Loss: 0.18\n",
      "Epoch 37: Train Loss: 0.1874, Val MRSE Loss: 0.1602, Val MAPE Loss: 0.319, Val WroPerc: 0.2254\n",
      "Batch 40, MRSE Loss: 0.19\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.18\n",
      "Batch 320, MRSE Loss: 0.19\n",
      "Epoch 38: Train Loss: 0.1726, Val MRSE Loss: 0.1674, Val MAPE Loss: 0.3268, Val WroPerc: 0.2289\n",
      "Batch 40, MRSE Loss: 0.19\n",
      "Batch 80, MRSE Loss: 0.18\n",
      "Batch 120, MRSE Loss: 0.18\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.19\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.19\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 39: Train Loss: 0.1706, Val MRSE Loss: 0.1523, Val MAPE Loss: 0.3086, Val WroPerc: 0.2148\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.19\n",
      "Batch 120, MRSE Loss: 0.19\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.17\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 40: Train Loss: 0.1694, Val MRSE Loss: 0.1798, Val MAPE Loss: 0.3489, Val WroPerc: 0.243\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.17\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.17\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 41: Train Loss: 0.1649, Val MRSE Loss: 0.1624, Val MAPE Loss: 0.3221, Val WroPerc: 0.2359\n",
      "Batch 40, MRSE Loss: 0.21\n",
      "Batch 80, MRSE Loss: 0.18\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.22\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 42: Train Loss: 0.1736, Val MRSE Loss: 0.1677, Val MAPE Loss: 0.3251, Val WroPerc: 0.2324\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.18\n",
      "Batch 120, MRSE Loss: 0.18\n",
      "Batch 160, MRSE Loss: 0.18\n",
      "Batch 200, MRSE Loss: 0.20\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.18\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 43: Train Loss: 0.1713, Val MRSE Loss: 0.1624, Val MAPE Loss: 0.3214, Val WroPerc: 0.2183\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.17\n",
      "Batch 280, MRSE Loss: 0.12\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 44: Train Loss: 0.1580, Val MRSE Loss: 0.1528, Val MAPE Loss: 0.3121, Val WroPerc: 0.2077\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.18\n",
      "Batch 200, MRSE Loss: 0.19\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.18\n",
      "Epoch 45: Train Loss: 0.1632, Val MRSE Loss: 0.1459, Val MAPE Loss: 0.3021, Val WroPerc: 0.2077\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.17\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.18\n",
      "Batch 320, MRSE Loss: 0.19\n",
      "Epoch 46: Train Loss: 0.1635, Val MRSE Loss: 0.1806, Val MAPE Loss: 0.3485, Val WroPerc: 0.2535\n",
      "Batch 40, MRSE Loss: 0.20\n",
      "Batch 80, MRSE Loss: 0.12\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 47: Train Loss: 0.1561, Val MRSE Loss: 0.1859, Val MAPE Loss: 0.3423, Val WroPerc: 0.243\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.18\n",
      "Batch 160, MRSE Loss: 0.19\n",
      "Batch 200, MRSE Loss: 0.18\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 48: Train Loss: 0.1550, Val MRSE Loss: 0.1515, Val MAPE Loss: 0.3032, Val WroPerc: 0.2183\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.17\n",
      "Batch 120, MRSE Loss: 0.19\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.21\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 49: Train Loss: 0.1638, Val MRSE Loss: 0.1461, Val MAPE Loss: 0.3011, Val WroPerc: 0.2077\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.17\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.19\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 50: Train Loss: 0.1573, Val MRSE Loss: 0.1406, Val MAPE Loss: 0.2928, Val WroPerc: 0.1937\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.19\n",
      "Batch 200, MRSE Loss: 0.12\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.22\n",
      "Epoch 51: Train Loss: 0.1585, Val MRSE Loss: 0.1960, Val MAPE Loss: 0.3742, Val WroPerc: 0.2711\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.18\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.17\n",
      "Batch 280, MRSE Loss: 0.17\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 52: Train Loss: 0.1627, Val MRSE Loss: 0.1696, Val MAPE Loss: 0.3309, Val WroPerc: 0.25\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.19\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.18\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.19\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 53: Train Loss: 0.1573, Val MRSE Loss: 0.1563, Val MAPE Loss: 0.3135, Val WroPerc: 0.2183\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.19\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 54: Train Loss: 0.1617, Val MRSE Loss: 0.1567, Val MAPE Loss: 0.3143, Val WroPerc: 0.2218\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 55: Train Loss: 0.1608, Val MRSE Loss: 0.1588, Val MAPE Loss: 0.3171, Val WroPerc: 0.2218\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.18\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.12\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 56: Train Loss: 0.1558, Val MRSE Loss: 0.1413, Val MAPE Loss: 0.2927, Val WroPerc: 0.2042\n",
      "Batch 40, MRSE Loss: 0.19\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.18\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.18\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 57: Train Loss: 0.1661, Val MRSE Loss: 0.1403, Val MAPE Loss: 0.2884, Val WroPerc: 0.1972\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.12\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.17\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 58: Train Loss: 0.1548, Val MRSE Loss: 0.2046, Val MAPE Loss: 0.3902, Val WroPerc: 0.2676\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.19\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 59: Train Loss: 0.1563, Val MRSE Loss: 0.1969, Val MAPE Loss: 0.3739, Val WroPerc: 0.2923\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.20\n",
      "Batch 240, MRSE Loss: 0.17\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 60: Train Loss: 0.1571, Val MRSE Loss: 0.1435, Val MAPE Loss: 0.2913, Val WroPerc: 0.1972\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 61: Train Loss: 0.1556, Val MRSE Loss: 0.1346, Val MAPE Loss: 0.2787, Val WroPerc: 0.1761\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 62: Train Loss: 0.1518, Val MRSE Loss: 0.1402, Val MAPE Loss: 0.2897, Val WroPerc: 0.2007\n",
      "Batch 40, MRSE Loss: 0.19\n",
      "Batch 80, MRSE Loss: 0.17\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.19\n",
      "Batch 200, MRSE Loss: 0.18\n",
      "Batch 240, MRSE Loss: 0.17\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 63: Train Loss: 0.1633, Val MRSE Loss: 0.1417, Val MAPE Loss: 0.2903, Val WroPerc: 0.1796\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.19\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.12\n",
      "Epoch 64: Train Loss: 0.1547, Val MRSE Loss: 0.1458, Val MAPE Loss: 0.2902, Val WroPerc: 0.2113\n",
      "Batch 40, MRSE Loss: 0.11\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.20\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.20\n",
      "Batch 240, MRSE Loss: 0.18\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 65: Train Loss: 0.1624, Val MRSE Loss: 0.1629, Val MAPE Loss: 0.3241, Val WroPerc: 0.2394\n",
      "Batch 40, MRSE Loss: 0.12\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.20\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.18\n",
      "Batch 320, MRSE Loss: 0.18\n",
      "Epoch 66: Train Loss: 0.1587, Val MRSE Loss: 0.1559, Val MAPE Loss: 0.3167, Val WroPerc: 0.2148\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.19\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 67: Train Loss: 0.1556, Val MRSE Loss: 0.1373, Val MAPE Loss: 0.2857, Val WroPerc: 0.1866\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.17\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 68: Train Loss: 0.1525, Val MRSE Loss: 0.1649, Val MAPE Loss: 0.3248, Val WroPerc: 0.243\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.17\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 69: Train Loss: 0.1500, Val MRSE Loss: 0.1670, Val MAPE Loss: 0.3284, Val WroPerc: 0.243\n",
      "Batch 40, MRSE Loss: 0.18\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 70: Train Loss: 0.1535, Val MRSE Loss: 0.1398, Val MAPE Loss: 0.2763, Val WroPerc: 0.2007\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.13\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 71: Train Loss: 0.1435, Val MRSE Loss: 0.1372, Val MAPE Loss: 0.2697, Val WroPerc: 0.2077\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.18\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.13\n",
      "Batch 240, MRSE Loss: 0.11\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 72: Train Loss: 0.1417, Val MRSE Loss: 0.1915, Val MAPE Loss: 0.366, Val WroPerc: 0.2852\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.23\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 73: Train Loss: 0.1560, Val MRSE Loss: 0.1355, Val MAPE Loss: 0.2734, Val WroPerc: 0.1901\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.12\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 74: Train Loss: 0.1436, Val MRSE Loss: 0.1450, Val MAPE Loss: 0.298, Val WroPerc: 0.2007\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.11\n",
      "Batch 240, MRSE Loss: 0.16\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 75: Train Loss: 0.1451, Val MRSE Loss: 0.1302, Val MAPE Loss: 0.2714, Val WroPerc: 0.1761\n",
      "Batch 40, MRSE Loss: 0.11\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 76: Train Loss: 0.1422, Val MRSE Loss: 0.1728, Val MAPE Loss: 0.3325, Val WroPerc: 0.2535\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.12\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 77: Train Loss: 0.1396, Val MRSE Loss: 0.1604, Val MAPE Loss: 0.3099, Val WroPerc: 0.25\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.17\n",
      "Batch 320, MRSE Loss: 0.10\n",
      "Epoch 78: Train Loss: 0.1408, Val MRSE Loss: 0.1355, Val MAPE Loss: 0.2847, Val WroPerc: 0.1866\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.11\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.17\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 79: Train Loss: 0.1371, Val MRSE Loss: 0.1362, Val MAPE Loss: 0.2912, Val WroPerc: 0.1725\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.12\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 80: Train Loss: 0.1356, Val MRSE Loss: 0.1310, Val MAPE Loss: 0.2743, Val WroPerc: 0.1831\n",
      "Batch 40, MRSE Loss: 0.11\n",
      "Batch 80, MRSE Loss: 0.13\n",
      "Batch 120, MRSE Loss: 0.18\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.13\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.12\n",
      "Epoch 81: Train Loss: 0.1370, Val MRSE Loss: 0.1361, Val MAPE Loss: 0.2837, Val WroPerc: 0.1901\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.10\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 82: Train Loss: 0.1349, Val MRSE Loss: 0.1312, Val MAPE Loss: 0.2592, Val WroPerc: 0.1866\n",
      "Batch 40, MRSE Loss: 0.11\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.13\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 83: Train Loss: 0.1355, Val MRSE Loss: 0.1299, Val MAPE Loss: 0.2682, Val WroPerc: 0.1831\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.17\n",
      "Batch 160, MRSE Loss: 0.11\n",
      "Batch 200, MRSE Loss: 0.11\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.12\n",
      "Epoch 84: Train Loss: 0.1363, Val MRSE Loss: 0.1317, Val MAPE Loss: 0.2548, Val WroPerc: 0.1937\n",
      "Batch 40, MRSE Loss: 0.10\n",
      "Batch 80, MRSE Loss: 0.13\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.12\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.16\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 85: Train Loss: 0.1361, Val MRSE Loss: 0.1513, Val MAPE Loss: 0.2764, Val WroPerc: 0.25\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 86: Train Loss: 0.1344, Val MRSE Loss: 0.1295, Val MAPE Loss: 0.2653, Val WroPerc: 0.1866\n",
      "Batch 40, MRSE Loss: 0.08\n",
      "Batch 80, MRSE Loss: 0.12\n",
      "Batch 120, MRSE Loss: 0.15\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.14\n",
      "Epoch 87: Train Loss: 0.1343, Val MRSE Loss: 0.1381, Val MAPE Loss: 0.2669, Val WroPerc: 0.1972\n",
      "Batch 40, MRSE Loss: 0.10\n",
      "Batch 80, MRSE Loss: 0.16\n",
      "Batch 120, MRSE Loss: 0.16\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.11\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.12\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 88: Train Loss: 0.1360, Val MRSE Loss: 0.1439, Val MAPE Loss: 0.2527, Val WroPerc: 0.2289\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.11\n",
      "Batch 120, MRSE Loss: 0.11\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.17\n",
      "Epoch 89: Train Loss: 0.1339, Val MRSE Loss: 0.1380, Val MAPE Loss: 0.2549, Val WroPerc: 0.2148\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.17\n",
      "Batch 200, MRSE Loss: 0.13\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 90: Train Loss: 0.1381, Val MRSE Loss: 0.1392, Val MAPE Loss: 0.2908, Val WroPerc: 0.1972\n",
      "Batch 40, MRSE Loss: 0.17\n",
      "Batch 80, MRSE Loss: 0.12\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.14\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 91: Train Loss: 0.1340, Val MRSE Loss: 0.1292, Val MAPE Loss: 0.2637, Val WroPerc: 0.2007\n",
      "Batch 40, MRSE Loss: 0.14\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.16\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.11\n",
      "Epoch 92: Train Loss: 0.1356, Val MRSE Loss: 0.1294, Val MAPE Loss: 0.2602, Val WroPerc: 0.1901\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.11\n",
      "Batch 120, MRSE Loss: 0.10\n",
      "Batch 160, MRSE Loss: 0.15\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.11\n",
      "Batch 320, MRSE Loss: 0.16\n",
      "Epoch 93: Train Loss: 0.1352, Val MRSE Loss: 0.1711, Val MAPE Loss: 0.3256, Val WroPerc: 0.2394\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.13\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 94: Train Loss: 0.1330, Val MRSE Loss: 0.1315, Val MAPE Loss: 0.2593, Val WroPerc: 0.2007\n",
      "Batch 40, MRSE Loss: 0.13\n",
      "Batch 80, MRSE Loss: 0.11\n",
      "Batch 120, MRSE Loss: 0.13\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.20\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.11\n",
      "Batch 320, MRSE Loss: 0.13\n",
      "Epoch 95: Train Loss: 0.1386, Val MRSE Loss: 0.1403, Val MAPE Loss: 0.2508, Val WroPerc: 0.2254\n",
      "Batch 40, MRSE Loss: 0.12\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.11\n",
      "Batch 160, MRSE Loss: 0.16\n",
      "Batch 200, MRSE Loss: 0.13\n",
      "Batch 240, MRSE Loss: 0.15\n",
      "Batch 280, MRSE Loss: 0.13\n",
      "Batch 320, MRSE Loss: 0.11\n",
      "Epoch 96: Train Loss: 0.1325, Val MRSE Loss: 0.1409, Val MAPE Loss: 0.2563, Val WroPerc: 0.2148\n",
      "Batch 40, MRSE Loss: 0.12\n",
      "Batch 80, MRSE Loss: 0.15\n",
      "Batch 120, MRSE Loss: 0.14\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.13\n",
      "Batch 240, MRSE Loss: 0.13\n",
      "Batch 280, MRSE Loss: 0.11\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 97: Train Loss: 0.1317, Val MRSE Loss: 0.1315, Val MAPE Loss: 0.2467, Val WroPerc: 0.2042\n",
      "Batch 40, MRSE Loss: 0.16\n",
      "Batch 80, MRSE Loss: 0.13\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.14\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.14\n",
      "Batch 320, MRSE Loss: 0.12\n",
      "Epoch 98: Train Loss: 0.1352, Val MRSE Loss: 0.1303, Val MAPE Loss: 0.2518, Val WroPerc: 0.1972\n",
      "Batch 40, MRSE Loss: 0.15\n",
      "Batch 80, MRSE Loss: 0.12\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.12\n",
      "Batch 200, MRSE Loss: 0.12\n",
      "Batch 240, MRSE Loss: 0.14\n",
      "Batch 280, MRSE Loss: 0.15\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 99: Train Loss: 0.1325, Val MRSE Loss: 0.1333, Val MAPE Loss: 0.2497, Val WroPerc: 0.2042\n",
      "Batch 40, MRSE Loss: 0.18\n",
      "Batch 80, MRSE Loss: 0.14\n",
      "Batch 120, MRSE Loss: 0.12\n",
      "Batch 160, MRSE Loss: 0.10\n",
      "Batch 200, MRSE Loss: 0.15\n",
      "Batch 240, MRSE Loss: 0.12\n",
      "Batch 280, MRSE Loss: 0.11\n",
      "Batch 320, MRSE Loss: 0.15\n",
      "Epoch 100: Train Loss: 0.1333, Val MRSE Loss: 0.1426, Val MAPE Loss: 0.2542, Val WroPerc: 0.2324\n",
      "Test MRSE Loss: 0.1559, Test MAPE Loss: 0.2768, Test WroPerc: 0.243\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    mrse_loss, mape_loss, wroperc_loss = train(train_loader)\n",
    "    v_mrse_loss, v_mape_loss, v_wroperc_loss = test(valid_loader)\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {mrse_loss:.4f}, Val MRSE Loss: {v_mrse_loss:.4f}, Val MAPE Loss: {v_mape_loss:.4}, Val WroPerc: {v_wroperc_loss:.4}')\n",
    "\n",
    "t_mrse_loss, t_mape_loss, t_wroperc_loss = test(test_loader)\n",
    "print(f'Test MRSE Loss: {t_mrse_loss:.4f}, Test MAPE Loss: {t_mape_loss:.4}, Test WroPerc: {t_wroperc_loss:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reduce head to have better performance\n",
    "2. Run 100 epochs \n",
    "3. Use graph norm \n",
    "4. See how coupledgnn data is \n",
    "5. see if there is an issue with the y and the output\n",
    "\n",
    "Lab meetin 5/04 \n",
    "1. Remove graph pooling and just sum the nx1 matrix after decoding. After decoding apply sigmoid. \n",
    "2. Download weibo dataset and preprocess\n",
    "3. Remove activation between layers. Only needed at the end. \n",
    "4. make sure code is right because it is only possible to get fluctutation if your code is not right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
