{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import os   \n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from cap_dataset import CascadeRegression\n",
    "from cap_model import GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240417\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/dev/Repos/CAPP\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_regression_ds = CascadeRegression(root=\"data\", name=\"facebook\", edge_index_path=\"data/raw/facebook/adj.txt\", task=\"regression\", observation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/miniconda3/envs/ml/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CascadeRegression \n",
       "network: facebook \n",
       "task: regression \n",
       "observation-window t=2 \n",
       "(Number of graphs: 2000, Number of features [activation, deg cent, eigen cent, btwns cemt]: 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_regression_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4039, 4], edge_index=[2, 176468], y=[1], cascade_name='0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_regression_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 5.9435e-03, 3.1943e-06, 6.5754e-06])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_regression_ds[0].x[1660]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    0,  ..., 4038, 4031, 4038],\n",
       "        [   1,    0,    2,  ..., 4027, 4038, 4031]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_regression_ds.edge_index_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 8.6677e-03, 1.4799e-03, 5.4343e-07])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normalize the values\n",
    "fb_regression_ds[10].x[2274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/miniconda3/envs/ml/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = GAT(fb_regression_ds.num_features, 64, 1, 8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rezatabrizi/miniconda3/envs/ml/lib/python3.8/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "total_size = len(fb_regression_ds)\n",
    "train_size = int(0.7 * total_size)\n",
    "valid_size = int(0.15 * total_size) \n",
    "test_size = total_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split (fb_regression_ds, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_loader.dataset[2].y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    total_loss = 0 \n",
    "    model.train()\n",
    "    print(len(loader))\n",
    "    for idx, data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        print(data.cascade_name)\n",
    "        print (out.shape)\n",
    "        print(out)\n",
    "        print(data.y.shape)\n",
    "        print(data.y)\n",
    "        print('\\n')\n",
    "        loss = F.mse_loss(out.squeeze().unsqueeze(0), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len (loader)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            loss = F.mse_loss(out, data.y.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "1587\n",
      "torch.Size([1, 1])\n",
      "tensor([[-134.3325]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([38.7530])\n",
      "\n",
      "\n",
      "11\n",
      "torch.Size([1, 1])\n",
      "tensor([[73.9542]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([28.4530])\n",
      "\n",
      "\n",
      "1021\n",
      "torch.Size([1, 1])\n",
      "tensor([[248.1188]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([2.7980])\n",
      "\n",
      "\n",
      "1174\n",
      "torch.Size([1, 1])\n",
      "tensor([[250.4686]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([17.4680])\n",
      "\n",
      "\n",
      "721\n",
      "torch.Size([1, 1])\n",
      "tensor([[134.3540]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([4.3710])\n",
      "\n",
      "\n",
      "801\n",
      "torch.Size([1, 1])\n",
      "tensor([[-40.7121]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([4.7860])\n",
      "\n",
      "\n",
      "125\n",
      "torch.Size([1, 1])\n",
      "tensor([[-175.3950]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([12.3660])\n",
      "\n",
      "\n",
      "349\n",
      "torch.Size([1, 1])\n",
      "tensor([[-187.6257]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([5.4860])\n",
      "\n",
      "\n",
      "890\n",
      "torch.Size([1, 1])\n",
      "tensor([[-101.5634]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([6.8930])\n",
      "\n",
      "\n",
      "14\n",
      "torch.Size([1, 1])\n",
      "tensor([[41.9339]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([12.4470])\n",
      "\n",
      "\n",
      "232\n",
      "torch.Size([1, 1])\n",
      "tensor([[157.7515]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([27.2220])\n",
      "\n",
      "\n",
      "897\n",
      "torch.Size([1, 1])\n",
      "tensor([[186.7174]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([8.5330])\n",
      "\n",
      "\n",
      "1862\n",
      "torch.Size([1, 1])\n",
      "tensor([[99.6786]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([3.2700])\n",
      "\n",
      "\n",
      "1261\n",
      "torch.Size([1, 1])\n",
      "tensor([[-24.6840]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([17.4130])\n",
      "\n",
      "\n",
      "252\n",
      "torch.Size([1, 1])\n",
      "tensor([[-106.0724]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([7.4940])\n",
      "\n",
      "\n",
      "321\n",
      "torch.Size([1, 1])\n",
      "tensor([[-126.7914]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([11.7100])\n",
      "\n",
      "\n",
      "667\n",
      "torch.Size([1, 1])\n",
      "tensor([[-51.2057]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([26.9620])\n",
      "\n",
      "\n",
      "1462\n",
      "torch.Size([1, 1])\n",
      "tensor([[54.2511]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([7.8010])\n",
      "\n",
      "\n",
      "1384\n",
      "torch.Size([1, 1])\n",
      "tensor([[114.5158]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([3.5330])\n",
      "\n",
      "\n",
      "688\n",
      "torch.Size([1, 1])\n",
      "tensor([[117.0238]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([1.])\n",
      "\n",
      "\n",
      "621\n",
      "torch.Size([1, 1])\n",
      "tensor([[48.6676]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([4.8990])\n",
      "\n",
      "\n",
      "61\n",
      "torch.Size([1, 1])\n",
      "tensor([[-41.5470]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([20.1150])\n",
      "\n",
      "\n",
      "1351\n",
      "torch.Size([1, 1])\n",
      "tensor([[-97.9896]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([17.9210])\n",
      "\n",
      "\n",
      "1464\n",
      "torch.Size([1, 1])\n",
      "tensor([[-60.5897]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([4.2840])\n",
      "\n",
      "\n",
      "1098\n",
      "torch.Size([1, 1])\n",
      "tensor([[7.0912]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([7.2550])\n",
      "\n",
      "\n",
      "1810\n",
      "torch.Size([1, 1])\n",
      "tensor([[67.6281]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([2.5680])\n",
      "\n",
      "\n",
      "769\n",
      "torch.Size([1, 1])\n",
      "tensor([[79.0141]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([3.8540])\n",
      "\n",
      "\n",
      "503\n",
      "torch.Size([1, 1])\n",
      "tensor([[49.5656]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([9.4590])\n",
      "\n",
      "\n",
      "1026\n",
      "torch.Size([1, 1])\n",
      "tensor([[-15.5455]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([25.2320])\n",
      "\n",
      "\n",
      "531\n",
      "torch.Size([1, 1])\n",
      "tensor([[-34.6810]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([29.0390])\n",
      "\n",
      "\n",
      "1702\n",
      "torch.Size([1, 1])\n",
      "tensor([[-23.8375]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "tensor([22.0580])\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m test(valid_dataset)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/_tensor.py:534\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    526\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    527\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    532\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 534\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/autograd/graph.py:767\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    train_loss = train(train_dataset)\n",
    "    valid_loss = test(valid_dataset)\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "test_loss = test(test_loader)\n",
    "print(f'Test MSE Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reduce head to have better performance\n",
    "2. Run 100 epochs \n",
    "3. Use graph norm \n",
    "4. See how coupledgnn data is \n",
    "5. see if there is an issue with the y and the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
